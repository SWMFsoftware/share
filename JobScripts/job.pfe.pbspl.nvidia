#!/bin/tcsh

# WARNING !!! The compilation must be done on the same compute node !!!
#         !!! where the run is going to be performed                !!!

# Job script for Pleiades computer at NASA Ames using the GPU nodes.
# Currently the GPU nodes are on pbspl4, so it is best to
# 
#   ssh pbspl4
#   cd SWMF/run
#   qsub job.pfe.pbspl.nvidia
#
# An alternative is to submit jobs for different CPU types and
# keep the first one that started:
#
#   qsub.pfe.pbspl.pl job.pfe.pbspl.nvidia
#
# To check the status of submitted jobs, use
#
# qstat -u YOUR_USER_NAME             # on pbspl4 node
# qstat v100@pbspl4 -u YOUR_USER_NAME # from other nodes
#

#PBS -S /bin/csh
#PBS -N GPU

################
#   V100 GPU   #
################

# Max 36 CPUs and 4 (8 on devel) GPUs for sky_gpu
# Using 10 CPU cores can speed up compilation
# Extra CPU cores can be used by non-GPU models of the SWMF
### PBS -l select=1:ncpus=10:ngpus=1:model=sky_gpu:mem=256GB 

# Max 48 CPUs and 4 GPUs for cas_gpu
# For multinode jobs ncpu and ngpu are usually the same (1 GPU/MPI process)
# mem=256GB is necessary for running on 4 GPUs per node. The default is too large.
### PBS -l select=16:ncpus=4:ngpus=4:model=cas_gpu:mem=256GB

# Select queue:
# Max 16 nodes, 4 gpus per node, 24 hours
### PBS -q v100@pbspl4

# Max 2 sky_gpu nodes with 8 GPUs per node between 8am and 5pm PST
### PBS -q devel@pbspl4

################
#   A100 GPU   #
################

# Max 32 nodes in prime time, 64 nodes in non-prime time, 4 gpus per node
# Note: it is necessary to specify memory. The default is too little.
#PBS -l select=2:model=mil_a100:ncpus=4:ngpus=4:mem=128GB

# Select queue:
# Max 32 nodes for 24 hours: prime time
#PBS -q p_gpu_normal@pbspl4

# Max 64 nodes for 12 hours: non-prime time
### PBS -q np_gpu_wide

# Max 2 nodes for 2 hours
### PBS -q gpu_devel 

################
#   Both GPU   #
################

# Use all nodes exclusively and distribute the CPUs and GPUs evenly among the nodes
### PBS -l place=scatter:excl

# Set walltime
#PBS -l walltime=00:10:00

# Merge output and error files
#PBS -j oe

# Send mails if error occurs
#PBS -m e

# Specify group (account charged) if necessary
### PBS -W group_list=s2994

# Load nvidia runtime libraries (should be the same as used for compilation)
module purge
module use -a /nasa/nvidia/hpc_sdk/toss4/modulefiles
module load nvhpc-nompi/24.3
module use -a /nasa/modulefiles/testing
module load mpi-hpe/mpt.2.30

# This is needed for direct access to GPU memory with mpt
setenv MPI_USE_CUDA true

# To compile C and C++
setenv MPICC_CC nvc
setenv MPICXX_CXX nvc++

# (optional) set these to utilize memory pinning: 
### setenv MPI_DSM_VERBOSE
### setenv MPI_DSM_DISTRIBUTE

# these can probably be removed but need further testing:
#setenv CUDA_VISIBLE_DEVICES 0,1,2,3
#setenv MP_BIND yes

# cd into the run directory
cd $PBS_O_WORKDIR

# print node list into the output file
cat $PBS_NODEFILE

# run SWMF with proper number of CPU cores
mpiexec -n 8 ./SWMF.exe > runlog_`date +%y%m%d%H%M`

# compile some test on the compute node and run it
#make -j test_swpc_large_gpu NP=8 >& test_np8.log
