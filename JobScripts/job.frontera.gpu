#!/bin/bash

#!!!Important on MPI:
#The default command to run MPI applications is ibrun. However, we haven't 
#discovered any approach to make it work with nvhpc yet. BATSRUS has to be 
#compiled with openmpi, which comes with nvhpc. To run BATSRUS, one has to use
#mpiexec instead of ibrun.
#It is the safest to unload the impi and intel modules, otherwise they 
#will/may interfere with compiler settings (nvfortran, nvc, nvc++) and mpiexec.

# Use default modules. To use IDL,
# module load idl
#
# Compilation:
# to avoid using too many threads on the login node, compile code with 
# make -j 4
#
# Move run directory to the $SCRATCH disk:
# make rundir
# mv run $SCRATCH/run1
# ln -s $SCRATCH/run1 .
#
# submit the jobscript:
# sbatch job.frontera.gpu
#
# monitor jobs:
# squeue -u USERNAME
#
# Continuous post-processing has to be done on the compute nodes with
# sbatch job.postproc.frontera

#SBATCH -J test               # Job name
#SBATCH -o test.o%j           # Name of stdout output file
#SBATCH -e test.e%j           # Name of stderr error file
#SBATCH -p rtx                # Queue (partition) name: rtx for GPU
#SBATCH -N 1                  # Total # of nodes 
#SBATCH --tasks-per-node 4    # Number of MPI tasks per node (4 max). 
#SBATCH -t 00:10:00           # Run time (hh:mm:ss)
#SBATCH --mail-type=all       # Send email at begin and end of job
### SBATCH --mail-user=example@umich.edu
#SBATCH -A BCS21001           # Project/Allocation name (may need change)

# Any other commands must follow all #SBATCH directives...

# By 05/2020, the default MPI_Reduce algorithm in IMPI may
# produce wrong results. For example, reducing a large
# array of ~30,000 integers on 2000 MPIs does not work correctly. This bug
# can be avoided by using another MPI_Reduce algorithm. The environment
# variable I_MPI_ADJUST_REDUCE is used to select the algorithm, and 
# either I_MPI_ADJUST_REDUCE=1 or I_MPI_ADJUST_REDUCE=3 works. The
# following line can also be copied to .bashrc to select algorithm for
# an interactive session:
export I_MPI_ADJUST_REDUCE=1 

# IMPI always produces warning messages for the jobs with >128 MPIs.
# The following line suppresses such warnings. 
export UCX_LOG_LEVEL=ERROR 

# If there is error messages like
# ib_mlx5_log.c:174 Transport retry count exceeded on
#    mlx5_1:1/RoCE (synd 0x15 vend 0x81 hw_synd 0/0)
# add the following setting:
# export UCX_TLS="knem,rc"

# force nvfortran to use its own mpi library
# !!!use the following module commands for compilation with nvfortran:
module purge
ml use /scratch1/projects/compilers/nvhpc24v5/modulefiles
ml load nvhpc-hpcx/24.5
ml load gcc # this is needed for hpcx

######## The following comments are probably obsoleted ##############
# -hpcx gives "UCX  ERROR gdr_pin_buffer failed." for large runs. 
# online solution to set UCX_TLS=^gdr_copy doesn't work.
# use -openmpi3 instead:
# ml load nvhpc-openmpi3/24.5
#####################################################################

# To suppress the 'gdr_copy_md.c:139 UCX ERROR gdr_pin_buffer failed' error,
# the following line works. See the following webpage for more details: 
#  https://docs.nvidia.com/networking/display/hpcxv2210/known+issues
export UCX_TLS=^gdr_copy

# module list

# Launch MPI code... 
# change -n nGpus for # of gpus 
mpirun -n 2 ./BATSRUS.exe  > runlog

# Add node list to output:
echo $SLURM_NODELIST >> runlog
# mpirun -n 1 ./BATSRUS.exe  > runlog_`date +%y%m%d%H%M`

# The first node running ./PostProc.pl
# ibrun -n 1 ./PostProc.pl -r=300 -n=10 >& PostProc.log &

# the remianing nodes running SWMF
# Remember to change the number of processors if -N is changed
# Based on the system admin, task_affinity might be needed
# if PostProc.pl and SWMF.exe share the same node, but with
# -o (offset) 56, they should not share the same node.
# But if the performance is greatly reduced, then task_affinity
# is suggested to add ahead of ./SWMF.exe.

# ibrun -n 3528 -o 56 ./SWMF.exe > runlog_`date +%y%m%d%H%M`

#sleep 60
#touch PostProc.STOP
#wait
