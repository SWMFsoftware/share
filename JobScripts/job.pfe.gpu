#!/bin/tcsh
# Job script for Pleiades computer at NASA Ames using the GPU nodes.
# Currently the GPU nodes are on pbspl4, so it is best to
# 
#   ssh pbspl4
#   cd SWMF/run
#   qsub job.pfe.nvidia
#
# An alternative is to submit jobs for different CPU types and
# keep the first one that started:
#
#   qsub.pfe.pl job.pfe.nvidia GPU gpu_sky gpu_cas
#
# To check the status of submitted jobs, use
#
# qstat -u YOUR_USER_NAME             # on pbspl4 node
# qstat v100@pbspl4 -u YOUR_USER_NAME # from other nodes
#

#PBS -S /bin/csh
#PBS -N GPU

# To run on v100 GPU (up to 16 nodes, 4 gpus per node)
# mem=256GB is not necessary
### PBS -lselect=1:ncpus=2:ngpus=1:model=sky_gpu
#PBS -l select=16:model=cas_gpu:ncpus=4:ngpus=4:mem=256GB
#PBS -l place=scatter:excl

# Select queue
#PBS -q v100@pbspl4
### PBS -q devel@pbspl4

# A100 queues are not accessible without separate allocation
### PBS -q gpu_36h_16n@pbspl4

# Set walltime
#PBS -l walltime=00:10:00

# Merge output and error files
#PBS -j oe

# Send mails if error occurs
#PBS -m e

# Specify group (account charged) if necessary
### PBS -W group_list=s2994

# Load nvidia runtime libraries (should be the same as used for compilation)
module purge
module use -a /nasa/nvidia/hpc_sdk/toss4/modulefiles
module use -a /nasa/modulefiles/testing
# compile with nvhpc-nompi(need to test other versions)
module load nvhpc-nompi/24.1
# mpi-hpe/mpt works the best, but this openmpi also works:
### module load openmpi/4.1.6-toss4-nv24.1
module load mpi-hpe/mpt

# set these for direct access to GPU memory with mpt:
setenv MPI_USE_CUDA true
setenv MPI_DSM_VERBOSE
setenv MPI_DSM_DISTRIBUTE

# these can probably be removed but need further testing:
setenv CUDA_VISIBLE_DEVICES 0,1,2,3
setenv OMPI_FC nvfortran
setenv MP_BIND yes
setenv OMP_DYNAMIC FALSE
setenv OMP_PROC_BIND true
setenv OMP_PLACES cores
setenv OMP_NUM_THREADS 1

# cd into the run directory
cd $PBS_O_WORKDIR

# print node list into the output file
cat $PBS_NODEFILE

# run SWMF with proper number of CPU cores
cp PARAM.in.np64 PARAM.in

# use this for openmpi, which requires:
# turning off vader with --mca ..., or mpiexec complains about missing library
# there is an option to pin memory for better perfomance: (sometimes openmpi does this by default)
# memory pinning with --map-by ...

### mpiexec --mca btl tcp,self -np 64 --map-by numa --bind-to core --report-bindings --hostfile $PBS_NODEFILE ./BATSRUS.exe > runlog.np64
### mpiexec --mca btl ^vader -np 64 --map-by numa --bind-to core --report-bindings --hostfile $PBS_NODEFILE ./BATSRUS.exe > runlog.np64

# use this for mpt:
mpiexec -np 64 ./BATSRUS.exe > runlog.np64

exit
